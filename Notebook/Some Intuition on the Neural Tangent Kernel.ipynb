{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> <font color='black'> <b>Một số minh họa về Neural Tangent Kernel\r\n",
    "\r\n",
    "### Neural tangent kernels là một công cụ hữu ích để hiểu về quá trình huấn luyện mạng nơ-ron và sự chính quy hóa tiềm ẩn (implicit regularization) trong quá trình giảm dần độ dốc (gradient descent).\r\n",
    "### Trong bài viết này, ta sẽ minh họa khái niệm về neural tangent kernels thông qua một ví dụ hồi quy 1 chiều đơn giản."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <b> <font color='black'> Ví dụ 1: Khởi động\r\n",
    "---\r\n",
    "### Hãy bắt đầu từ ví dụ sau. Giả sử chúng ta có một hàm được xác định trên các số nguyên từ -10 đến 20. Chúng ta tham số hóa hàm này dưới dạng một bảng tra cứu, đó là giá trị của hàm $f(i)$ tại mỗi số nguyên $i$ được mô tả bởi một tham số $\\theta_i = f(i)$. Ta khởi tạo các tham số của hàm này là $\\theta_i = 3i+2$. Hàm này được biểu diễn bằng các chấm đen bên dưới:\r\n",
    "<center><img src=images/download-40.png></center>\r\n",
    "\r\n",
    "### Bây giờ, hãy xem xét điều gì sẽ xảy ra nếu chúng ta quan sát một điểm dữ liệu mới, $(x, y) = (10, 50)$, được hiển thị bằng dấu gạch chéo màu xanh lam. Chúng ta sẽ thực hiện cập nhật $\\theta$ bằng gradient descent. Giả sử chúng ta sử dụng hàm loss sai số bình phương $(f(10; \\theta) - 50)^2$ và tốc độ học (learning rate) $\\eta=0.1$. Vì giá trị của hàm tại $x = 10$ chỉ phụ thuộc vào một trong các tham số là $\\theta_{10}$, nên chỉ tham số này sẽ được cập nhật. Phần còn lại của các tham số, và do đó phần còn lại của các giá trị hàm không thay đổi. Các mũi tên màu đỏ minh họa cách các giá trị hàm di chuyển trong một bước gradient descent duy nhất: Hầu hết các giá trị hoàn toàn không di chuyển, chỉ một trong số chúng di chuyển gần hơn với dữ liệu được quan sát. Do đó, chỉ có một mũi tên màu đỏ có thể nhìn thấy được.\r\n",
    "### Tuy nhiên, trong học máy, chúng ta hiếm khi tham số hóa các hàm dưới dạng bảng tra cứu các giá trị hàm riêng lẻ. Việc tham số hóa này khá vô dụng vì nó không cho phép bạn nội suy chứ chưa nói đến việc ngoại suy cho dữ liệu không nhìn thấy. Hãy xem điều gì xảy ra trong một mô hình quen thuộc hơn: hồi quy tuyến tính."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <b> <font color='black'> Ví dụ 2: Hàm tuyến tính (Linear function)\r\n",
    "---\r\n",
    "### Bây giờ chúng ta hãy xem xét hàm tuyến tính $f(x, \\theta) = \\theta_1 x + \\theta_2$. Ta khởi tạo các tham số là $\\theta_1=3$ và $\\theta_2=1$, vì vậy khi khởi tạo, ta có chính xác hàm trên số nguyên như ta đã có trong ví dụ đầu tiên. Hãy xem điều gì xảy ra với hàm này khi ta cập nhật $\\theta$ bằng cách thực hiện một bước gradient descent và kết hợp với quan sát điểm $(x, y) = (10, 50)$ như trước. Một lần nữa, các mũi tên màu đỏ hiển thị cách mà các giá trị hàm di chuyển:\r\n",
    "<center><img src=images/download-37.png></center>\r\n",
    "\r\n",
    "### Chà! Chuyện gì đang diễn ra? Vì các giá trị hàm riêng lẻ không còn được tham số độc lập nữa, nên chúng ta không thể di chuyển chúng một cách độc lập. Mô hình liên kết chúng với nhau thông qua các tham số tổng thể $\\theta_1$ và $\\theta_2$. Nếu chúng ta muốn di chuyển hàm đến gần đầu ra mong muốn $y = 50$ tại vị trí $x = 10$ thì các giá trị hàm ở nơi khác cũng phải thay đổi.\r\n",
    "### Trong trường hợp này, việc cập nhật hàm với một điểm quan sát tại $x = 10$ sẽ làm thay đổi giá trị của hàm ở xa điểm quan sát. Nó thậm chí còn thay đổi giá trị hàm theo hướng ngược lại so với những gì chúng ta mong đợi .. Điều này có vẻ hơi kỳ lạ, nhưng đó thực sự là cách hoạt động của các mô hình tuyến tính.\r\n",
    "### Bây giờ chúng ta có một chút thông tin cơ bản để bắt đầu nói về neural tangent kernel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <b> <font color='black'> Liên hệ với neural tangent kernel\r\n",
    "---\r\n",
    "### Cho một hàm $f_\\theta(x)$ được tham số hóa bởi $\\theta$, *nhân tiếp tuyến thần kinh (neural tangent kernel)* của nó $k_\\theta(x, x')$ xác định giá trị của hàm tại $x$ thay đổi bao nhiêu khi chúng ta lấy một bước gradient nhỏ trong thời gian ngắn trong $\\theta$ kết hợp một quan sát mới tại $x'$. Một cách khác của cụm từ này là: $k(x, x')$ đo độ nhạy của giá trị hàm tại $x$ đối với các lỗi dự đoán tại $x'$.\r\n",
    "### Trong hình vẽ trên, kích thước của các mũi tên màu đỏ tại mỗi vị trí $x$ được cho bởi phương trình sau:\r\n",
    "### $$\\eta \\tilde{k}_\\theta(x, x') = f\\left(x, \\theta + \\eta \\frac{f_\\theta(x')}{d\\theta}\\right) - f(x, \\theta)$$\r\n",
    "### Theo cách nói của mạng nơ-ron, đây là những gì đang xảy ra: Hàm loss yêu cầu ta tăng giá trị hàm $f_\\theta(x')$. Ta lan truyền ngược (back-propagate) nó thông qua mạng để xem ta phải thực hiện thay đổi gì để đạt được điều này. Tuy nhiên, khi di chuyển $f_\\theta(x')$ theo cách này cũng đồng thời di chuyển $f_\\theta(x)$ tại các vị trí khác $x \\neq x'$. $\\tilde{k}_\\theta(x, x')$ biểu thị giá trị đó bằng bao nhiêu.\r\n",
    "### Neural kernel về cơ bản là một cái gì đó giống như giới hạn của $\\tilde{k}$ khi kích thước bước trở nên nhỏ vô cùng. Đặc biệt:\r\n",
    "### $$k(x, x') = \\lim_{\\eta \\rightarrow 0} \\frac{f\\left(x, \\theta + \\eta \\frac{df_\\theta(x')}{d\\theta}\\right) - f(x, \\theta)}{\\eta}$$\r\n",
    "### Sử dụng khai triển Taylor bậc 1 của $f_\\theta(x)$, có thể chỉ ra rằng\r\n",
    "### $$k_\\theta(x, x') = \\left\\langle \\frac{df_\\theta(x)}{d\\theta} , \\frac{f_\\theta(x')}{d\\theta} \\right\\rangle$$\r\n",
    "### Bài tập về nhà cho bạn: tìm $k(x, x')$ và / hoặc $\\tilde{k}(x, x')$ cho một $\\eta$ cố định trong mô hình tuyến tính trong ví dụ trước. Nó có tuyến tính không? Hay nó là cái gì khác?\r\n",
    "### Lưu ý rằng đây là một suy luận khác với những gì trong bài viết (which starts from continuous differential equation version of gradient descent).\r\n",
    "### Bây giờ, ta sẽ quay lại các ví dụ để minh họa hai thuộc tính quan trọng hơn của kernel này: độ nhạy đối với tham số hóa và những thay đổi trong quá trình huấn luyện."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <b> <font color='black'> Ví dụ 3: Mô hình tuyến tính được tham số lại (Reparametrized linear model)\r\n",
    "---\r\n",
    "### Ai cũng biết rằng mạng nơ-ron có thể được tham số hóa lại (repararmetized) theo những cách mà không làm thay đổi kết quả đầu ra thực tế của hàm, nhưng có thể dẫn đến sự khác biệt trong cách tối ưu hóa hoạt động. [Batchnorm](https://arxiv.org/pdf/1502.03167.pdf) là một ví dụ nổi tiếng về điều này. Chúng ta có thể thấy ảnh hưởng của việc tái tham số hóa trong neural tangent kernel không? Có, chúng ta có thể. Hãy xem điều gì sẽ xảy ra nếu ta tham số hóa lại hàm tuyến tính mà ta đã sử dụng trong ví dụ thứ hai như:\r\n",
    "### $$f_\\theta(x) = \\theta_1 x + \\color{blue}{10\\cdot}\\theta_2$$\r\n",
    "### nhưng bây giờ với các tham số $\\theta_1=3, \\theta_2=\\color{blue}{0.1}$. Ta đánh dấu màu xanh lam những gì đã thay đổi. Bản thân hàm, lúc khởi tạo giống nhau vì $10 * 0.1 = 1$. Lớp hàm cũng vậy, nên ta vẫn có thể triển khai các hàm tuyến tính tùy ý. Tuy nhiên, khi chúng ta xem xét ảnh hưởng của một bước gradient đơn lẻ, chúng ta thấy rằng hàm thay đổi khác nhau khi quá trình gradient descent được thực hiện trong tham số này.\r\n",
    "<center><img src=images/download-39.png></center>\r\n",
    "\r\n",
    "### Trong sự tham số hóa này, việc thực hiện gradient descent trở nên dễ dàng hơn để đẩy toàn bộ hàm lên bởi một hằng số, trong khi ở tham số trước đó, nó quyết định thay đổi độ dốc. Điều này chứng tỏ rằng hạt nhân tiếp tuyến thần kinh (neural tangent kernel) $k_\\theta(x, x')$ nhạy cảm với sự tham số hóa lại."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <b> <font color='black'> Ví dụ 4: Mạng hoạt động dựa trên cơ sở bán kính cực nhỏ (tiny radial basis function network)\r\n",
    "---\r\n",
    "### Mặc dù các mô hình tuyến tính có thể là minh họa tốt, nhưng chúng ta hãy xem $k_\\theta(x, x')$ trông như thế nào trong một mô hình phi tuyến. Ở đây, ta sẽ xem xét một mô hình có hai hàm cơ sở là mũ và bình phương:\r\n",
    "### $$f_\\theta(x) = \\theta_1 \\exp\\left(-\\frac{(x - \\theta_2)^2}{30}\\right) + \\theta_3 \\exp\\left(-\\frac{(x - \\theta_4)^2}{30}\\right)  + \\theta_5,$$\r\n",
    "### với các giá trị tham số ban đầu $(\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5) = (4.0, -10.0, 25.0, 10.0, 50.0)$. Chúng được chọn hơi tùy tiện và để làm cho kết quả trực quan:\r\n",
    "<center><img src=images/download-41.png></center>\r\n",
    "\r\n",
    "### Chúng ta có thể hình dung hàm $\\tilde{k}_\\theta(x, 10)$ một cách trực tiếp, thay vì vẽ nó trên đầu của hàm. Ở đây ta cũng chuẩn hóa nó bằng cách chia cho $\\tilde{k}_\\theta(10, 10)$.\r\n",
    "<center><img src=images/download-43.png></center>\r\n",
    "\r\n",
    "### Những gì chúng ta có thể thấy là nó bắt đầu giống một hàm *hạt nhân* ở chỗ có giá trị cao hơn gần $10$ và giảm khi bạn đi ra xa hơn. Tuy nhiên, có một số điều đáng chú ý: cực đại của hàm nhân này không phải ở $x = 10$, mà ở $x = 7$. Có nghĩa là giá trị hàm $f(7)$ thay đổi nhiều hơn trong phản ứng với một quan sát tại $x' = 10$ so với giá trị $f(10)$. Thứ hai, có một số giá trị âm. Trong trường hợp này, hình trước cung cấp một lời giải thích trực quan tại sao: chúng ta có thể tăng giá trị hàm tại $x = 10$ bằng cách đẩy thung lũng có tâm tại $\\theta_1=4$ ra khỏi nó, sang trái. Sự thay đổi tham số này đến lượt nó làm giảm các giá trị hàm trên bức tường bên trái của thung lũng. Thứ ba, hàm nhân hội tụ về một hằng số dương ở các đuôi của nó - điều này là do độ lệch $\\theta_5$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <b> <font color='black'> Ví dụ 5: Những thay đổi khi huấn luyện\r\n",
    "---\r\n",
    "### Bây giờ ta sẽ minh họa một thuộc tính quan trọng khác của hạt nhân tiếp tuyến thần kinh: nói chung, hạt nhân phụ thuộc vào giá trị tham số $\\theta$, và do đó nó thay đổi khi mô hình được huấn luyện. Ở đây ta chỉ ra những gì xảy ra với hạt nhân khi ta thực hiện 15 bước gradient ascent cố gắng tăng $f(10)$. Đường cong màu tím là đường ta có lúc khởi tạo (ở trên) và đường cong màu cam hiển thị hạt nhân ở bước gradient cuối cùng.\r\n",
    "<center><img src=images/download-45.png></center>\r\n",
    "\r\n",
    "### Các thay đổi tương ứng đối với các thay đổi của hàm $f_{\\theta_t}$ được hiển thị bên dưới:\r\n",
    "<center><img src=images/download-46.png></center>\r\n",
    "\r\n",
    "### Vì vậy, chúng ta có thể thấy rằng khi tham số thay đổi, hạt nhân cũng thay đổi. Nhân trở nên phẳng hơn. Một giải thích về điều này là cuối cùng chúng ta đạt đến một vùng không gian tham số, nơi mà $\\theta_4$ thay đổi nhanh nhất."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <b> <font color='black'> Tại sao điều này lại thú vị?\r\n",
    "---\r\n",
    "### Hóa ra hạt nhân tiếp tuyến thần kinh trở nên đặc biệt hữu ích khi nghiên cứu động lực học trong các mạng nơ-ron lan truyền tiến rộng vô hạn. Tại sao? Bởi vì trong giới hạn này, có hai điều xảy ra:\r\n",
    "1. ### Thứ nhất: nếu chúng ta khởi tạo $\\theta_0$ một cách ngẫu nhiên từ các phân phối được chọn phù hợp, NTK ban đầu của mạng $k_{\\theta_0}$ sẽ tiếp cận một hạt nhân xác định khi chiều rộng tăng lên. Điều này có nghĩa là lúc khởi tạo, $k_{\\theta_0}$ không thực sự phụ thuộc vào $\\theta_0$ mà là một hạt nhân cố định độc lập với lần khởi tạo cụ thể.\r\n",
    "2. ### Thứ hai: trong giới hạn vô hạn, nhân $k_{\\theta_t}$ không đổi theo thời gian khi chúng ta tối ưu hoá $\\theta_t$. Điều này loại bỏ sự phụ thuộc tham số trong quá trình huấn luyện.\r\n",
    "### Hai dữ kiện này kết hợp với nhau ngụ ý rằng gradient descent trong giới hạn tốc độ học rộng vô hạn và cực kỳ nhỏ có thể được hiểu như một thuật toán khá đơn giản được gọi là *kernel gradient descent* với một hàm kernel cố định chỉ phụ thuộc vào kiến ​​trúc (số lớp, hàm kích hoạt, v.v. ).\r\n",
    "### Những kết quả này, được thực hiện cùng với một kết quả đã biết cũ hơn của [Neal, (1994)](https://arxiv.org/pdf/1912.02803.pdf), cho phép chúng ta mô tả phân phối xác suất của cực tiểu mà gradient descent hội tụ đến trong giới hạn vô hạn này như một quá trình Gauss. Để biết chi tiết, hãy xem bài báo được đề cập ở trên."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <b> <font color='black'> Đừng nhầm lẫn giữa các hạt nhân\r\n",
    "---\r\n",
    "### Có hai bộ kết quả có phần liên quan, cả hai đều liên quan đến mạng lưới thần kinh rộng vô hạn (infinitely wide neural netwoks) và các hàm hạt nhân (kernel functions), ta làm rõ sự khác biệt giữa chúng:\r\n",
    "1. ### kết quả cũ hơn, nổi tiếng của [Neal, (1994)](https://arxiv.org/pdf/1912.02803.pdf), sau đó được mở rộng bởi những người khác, là phân phối $f_\\theta$ dưới sự khởi tạo ngẫu nhiên của $\\theta$ hội tụ với một quá trình Gaussian. Quá trình Gaussian này có một nhân hoặc hàm hiệp phương sai, nói chung, không giống với nhân tiếp tuyến thần kinh. Kết quả cũ này không nói lên bất cứ điều gì về gradient descent, và thường được sử dụng để thúc đẩy việc sử dụng các phương pháp Bayes dựa trên quy trình Gaussian.\r\n",
    "2. ### kết quả mới, NTK, là sự tiến hóa của $f_{\\theta_t}$ trong quá trình huấn luyện gradient descent có thể được mô tả dưới dạng hạt nhân, hạt nhân tiếp tuyến thần kinh, và trong giới hạn vô hạn, hạt nhân này không đổi trong quá trình huấn luyện và xác định khi khởi tạo. Sử dụng kết quả này, có thể chỉ ra rằng trong một số trường hợp, phân phối $f_{\\theta_t}$ là một quá trình Gaussian ở mọi bước thời gian $t$, không chỉ khi khởi tạo. Kết quả này cũng cho phép chúng ta xác định quá trình Gaussian mô tả giới hạn là $t \\rightarrow \\infty$. Tuy nhiên, quá trình Gaussian giới hạn này không giống với quá trình Gaussian sau mà Neal và những người khác sẽ tính toán trên cơ sở kết quả đầu tiên.\r\n",
    "### Hy vọng bài này đã giúp ích một chút cho mọi người bằng cách xây dựng một số minh họa về hạt nhân tiếp tuyến thần kinh là gì. Nếu bạn quan tâm, hãy xem [sổ tay colab](https://colab.research.google.com/drive/1zAMTtSuxR5V8xBJL02FMNtCLJuKY9TWg?usp=sharing) đơn giản đã được sử dụng cho những hình minh họa trong bài này."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "dcacb0086e9a4f4eabd41c33bf4faac5ea0a3337ed3f5eff0680afa930572c04"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}